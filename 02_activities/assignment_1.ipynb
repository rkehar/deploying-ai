{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f0928fd5",
   "metadata": {},
   "source": [
    "# Deploying AI\n",
    "## Assignment 1: Evaluating Summaries"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f3586e4",
   "metadata": {},
   "source": [
    "A key application of LLMs is to summarize documents. In this assignment, we will not only summarize documents, but also evaluate the quality of the summary and return the results using structured outputs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "609f2fa2",
   "metadata": {},
   "source": [
    "**Instructions:** please complete the sections below stating any relevant decisions that you have made and showing the code substantiating your solution."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "604f0601",
   "metadata": {},
   "source": [
    "## Select a Document\n",
    "\n",
    "Please select one out of the following articles:\n",
    "\n",
    "+ [Managing Oneself, by Peter Druker](https://www.thecompleteleader.org/sites/default/files/imce/Managing%20Oneself_Drucker_HBR.pdf)  (PDF)\n",
    "+ [The GenAI Divide: State of AI in Business 2025](https://www.artificialintelligence-news.com/wp-content/uploads/2025/08/ai_report_2025.pdf) (PDF)\n",
    "+ [What is Noise?, by Alex Ross](https://www.newyorker.com/magazine/2024/04/22/what-is-noise) (Web)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c125d1e",
   "metadata": {},
   "source": [
    "# Load Secrets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b8dbcc48",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext dotenv\n",
    "%dotenv ../05_src/.secrets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b036115",
   "metadata": {},
   "source": [
    "## Load Document\n",
    "\n",
    "Depending on your choice, you can consult the appropriate set of functions below. Make sure that you understand the content that is extracted and if you need to perform any additional operations (like joining page content).\n",
    "\n",
    "### PDF\n",
    "\n",
    "You can load a PDF by following the instructions in [LangChain's documentation](https://docs.langchain.com/oss/python/langchain/knowledge-base#loading-documents). Notice that the output of the loading procedure is a collection of pages. You can join the pages by using the code below.\n",
    "\n",
    "```python\n",
    "document_text = \"\"\n",
    "for page in docs:\n",
    "    document_text += page.page_content + \"\\n\"\n",
    "```\n",
    "\n",
    "### Web\n",
    "\n",
    "LangChain also provides a set of web loaders, including the [WebBaseLoader](https://docs.langchain.com/oss/python/integrations/document_loaders/web_base). You can use this function to load web pages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "256159db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 25.2 -> 25.3\n",
      "[notice] To update, run: C:\\Users\\romab\\AppData\\Local\\Microsoft\\WindowsApps\\PythonSoftwareFoundation.Python.3.13_qbz5n2kfra8p0\\python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of pages loaded: 26\n",
      "\n",
      "First 200 characters of first page:\n",
      "pg. 1 \n",
      " \n",
      " \n",
      "The GenAI Divide  \n",
      "STATE OF AI IN \n",
      "BUSINESS 2025 \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      "MIT NANDA \n",
      "Aditya Challapally \n",
      "Chris Pease \n",
      "Ramesh Raskar \n",
      "Pradyumna Chari \n",
      "July 2025\n",
      "\n",
      "Metadata of first page:\n",
      "{'producer': 'Microsoft® Word for Microsoft 365', 'creator': 'Microsoft® Word for Microsoft 365', 'creationdate': '2025-07-13T21:18:19-07:00', 'msip_label_87867195-f2b8-4ac2-b0b6-6bb73cb33afc_siteid': '72f988bf-86f1-41af-91ab-2d7cd011db47', 'msip_label_87867195-f2b8-4ac2-b0b6-6bb73cb33afc_method': 'Privileged', 'msip_label_87867195-f2b8-4ac2-b0b6-6bb73cb33afc_enabled': 'True', 'author': 'Aditya Challapally', 'moddate': '2025-07-13T21:18:19-07:00', 'source': 'ai_report_2025.pdf', 'total_pages': 26, 'page': 0, 'page_label': '1'}\n",
      "\n",
      "Total document length: 53851 characters\n"
     ]
    }
   ],
   "source": [
    "# Install dependencies (run once)\n",
    "%pip install -qU langchain-community pypdf\n",
    "\n",
    "# Load secrets / API key\n",
    "import getpass\n",
    "import os\n",
    "\n",
    "os.environ[\"LANGSMITH_TRACING\"] = \"true\"\n",
    "os.environ[\"LANGSMITH_API_KEY\"] = getpass.getpass()\n",
    "\n",
    "# Load a PDF\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "\n",
    "file_path = \"ai_report_2025.pdf\"  \n",
    "loader = PyPDFLoader(file_path)\n",
    "docs = loader.load()\n",
    "\n",
    "# Inspect results\n",
    "print(f\"Number of pages loaded: {len(docs)}\\n\")\n",
    "print(f\"First 200 characters of first page:\\n{docs[0].page_content[:200]}\\n\")\n",
    "print(f\"Metadata of first page:\\n{docs[0].metadata}\")\n",
    "\n",
    "# Join pages into single document text\n",
    "document_text = \"\"\n",
    "for page in docs:\n",
    "    document_text += page.page_content + \"\\n\"\n",
    "\n",
    "print(f\"\\nTotal document length: {len(document_text)} characters\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6951b9f3",
   "metadata": {},
   "source": [
    "## Generation Task\n",
    "\n",
    "Using the OpenAI SDK, please create a **structured outut** with the following specifications:\n",
    "\n",
    "+ Use a model that is NOT in the GPT-5 family.\n",
    "+ Output should be a Pydantic BaseModel object. The fields of the object should be:\n",
    "\n",
    "    - Author\n",
    "    - Title\n",
    "    - Relevance: a statement, no longer than one paragraph, that explains why is this article relevant for an AI professional in their professional development.\n",
    "    - Summary: a concise and succinct summary no longer than 1000 tokens.\n",
    "    - Tone: the tone used to produce the summary (see below).\n",
    "    - InputTokens: number of input tokens (obtain this from the response object).\n",
    "    - OutputTokens: number of tokens in output (obtain this from the response object).\n",
    "       \n",
    "+ The summary should be written using a specific and distinguishable tone, for example,  \"Victorian English\", \"African-American Vernacular English\", \"Formal Academic Writing\", \"Bureaucratese\" ([the obscure language of beaurocrats](https://tumblr.austinkleon.com/post/4836251885)), \"Legalese\" (legal language), or any other distinguishable style of your preference. Make sure that the style is something you can identify. \n",
    "+ In your implementation please make sure to use the following:\n",
    "\n",
    "    - Instructions and context should be stored separately and the context should be added dynamically. Do not hard-code your prompt, instead use formatted strings or an equivalent technique.\n",
    "    - Use the developer (instructions) prompt and the user prompt.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cc9ea64",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 25.2 -> 25.3\n",
      "[notice] To update, run: C:\\Users\\romab\\AppData\\Local\\Microsoft\\WindowsApps\\PythonSoftwareFoundation.Python.3.13_qbz5n2kfra8p0\\python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Raw model output preview:\n",
      " ```json\n",
      "{\n",
      "    \"Author\": \"Aditya Challapally\",\n",
      "    \"Title\": \"ai_report_2025.pdf\",\n",
      "    \"Relevance\": \"This article is highly relevant for AI professionals as it provides a comprehensive analysis of the current state and future trajectory of AI implementation in business. It identifies the critical challenges and opportunities associated with the GenAI Divide, offering insights into why many AI initiatives fail to deliver expected returns. By understanding these dynamics, AI professionals can better navigate the complexities of AI adoption, enhance their strategic decision-making, and align their efforts with successful practices that bridge the divide between high adoption and meaningful transformation.\",\n",
      "    \"Summary\": \"The report 'State of AI in Business 2025' by Aditya Challapally and othe\n",
      "\n",
      "Initial ArticleSummary (pydantic):\n",
      "{\n",
      "  \"Author\": \"Aditya Challapally\",\n",
      "  \"Title\": \"ai_report_2025.pdf\",\n",
      "  \"Relevance\": \"This article is highly relevant for AI professionals as it provides a comprehensive analysis of the current state and future trajectory of AI implementation in business. It identifies the critical challenges and opportunities associated with the GenAI Divide, offering insights into why many AI initiatives fail to deliver expected returns. By understanding these dynamics, AI professionals can better navigate the complexities of AI adoption, enhance their strategic decision-making, and align their efforts with successful practices that bridge the divide between high adoption and meaningful transformation.\",\n",
      "  \"Summary\": \"The report 'State of AI in Business 2025' by Aditya Challapally and others examines the GenAI Divide, a significant gap between AI adoption and transformation in enterprises. Despite substantial investments, only 5% of AI initiatives yield measurable returns, primarily due to a lack of learning and integration capabilities in AI systems. The report identifies four patterns contributing to this divide: limited disruption across industries, an enterprise paradox where large firms pilot but fail to scale, investment biases favoring visible functions, and the advantage of external partnerships over internal builds. Successful organizations demand process-specific customization and evaluate tools based on business outcomes. The report highlights the emergence of a 'shadow AI economy' where employees use personal AI tools for work, often yielding better ROI than official initiatives. It also discusses the importance of strategic partnerships and the need for AI systems that learn and adapt over time. The report concludes with a call for organizations to shift from static tools to adaptive systems, emphasizing the potential of the Agentic Web to transform business processes through autonomous coordination.\",\n",
      "  \"Tone\": \"Formal Academic Writing\",\n",
      "  \"InputTokens\": 10947,\n",
      "  \"OutputTokens\": 367\n",
      "}\n",
      "Saved ai_report_2025_initial_summary.json\n"
     ]
    }
   ],
   "source": [
    "# Install the libraries\n",
    "# Run once \n",
    "# %pip install -qU openai pydantic pypdf\n",
    "\n",
    "# Imports\n",
    "from openai import OpenAI\n",
    "from pydantic import BaseModel\n",
    "from pypdf import PdfReader\n",
    "import json\n",
    "\n",
    "# Client + model configuration\n",
    "client = OpenAI()  # Uses OPENAI_API_KEY from env\n",
    "GEN_MODEL = \"gpt-4o\"\n",
    "\n",
    "# Pydantic schema\n",
    "class ArticleSummary(BaseModel):\n",
    "    Author: str\n",
    "    Title: str\n",
    "    Relevance: str\n",
    "    Summary: str\n",
    "    Tone: str\n",
    "    InputTokens: int\n",
    "    OutputTokens: int\n",
    "\n",
    "# Load PDF document\n",
    "def load_pdf(filepath):\n",
    "    \"\"\"Extract text from PDF file\"\"\"\n",
    "    reader = PdfReader(filepath)\n",
    "    return \"\\n\".join([page.extract_text() for page in reader.pages])\n",
    "\n",
    "# Load your document\n",
    "document_text = load_pdf(\"ai_report_2025.pdf\")\n",
    "\n",
    "# Developer instructions (system prompt)\n",
    "developer_instructions = (\n",
    "    \"You are a high-precision summarization assistant. \"\n",
    "    \"Return only valid JSON containing EXACTLY these keys: \"\n",
    "    '\"Author\", \"Title\", \"Relevance\", \"Summary\", \"Tone\", \"InputTokens\", \"OutputTokens\". '\n",
    "    \"Follow the user's tone instruction exactly. \"\n",
    "    \"'Relevance' should be one paragraph. \"\n",
    "    \"'Summary' should be concise and no longer than 1000 tokens.\"\n",
    ")\n",
    "\n",
    "# Configuration (change these as needed)\n",
    "chosen_tone = \"Formal Academic Writing\"  # or 'Victorian English', 'Legalese', etc.\n",
    "article_title = \"ai_report_2025.pdf\"\n",
    "article_author = \"Aditya Challapally\"\n",
    "\n",
    "# Build user prompt dynamically\n",
    "user_prompt = f\"\"\"\n",
    "Article author: {article_author}\n",
    "Article title: {article_title}\n",
    "Tone for the SUMMARY: {chosen_tone}\n",
    "\n",
    "Produce a valid JSON object with these keys:\n",
    "- Author: the article author\n",
    "- Title: the article title\n",
    "- Relevance: one paragraph explaining why this article is relevant for an AI professional's professional development\n",
    "- Summary: concise summary (max 1000 tokens) written in {chosen_tone}\n",
    "- Tone: the tone you used (string)\n",
    "- InputTokens: set to 0 (will be updated)\n",
    "- OutputTokens: set to 0 (will be updated)\n",
    "\n",
    "ARTICLE START\n",
    "{document_text}\n",
    "ARTICLE END\n",
    "\n",
    "Output only the JSON object, nothing else.\n",
    "\"\"\"\n",
    "\n",
    "# Call OpenAI Chat Completions API\n",
    "resp = client.chat.completions.create(\n",
    "    model=GEN_MODEL,\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": developer_instructions},\n",
    "        {\"role\": \"user\", \"content\": user_prompt}\n",
    "    ],\n",
    "    response_format={\"type\": \"json_object\"},  # Enforces JSON output\n",
    "    max_tokens=1200,\n",
    "    temperature=0.2,\n",
    ")\n",
    "\n",
    "# Extract response\n",
    "model_text = resp.choices[0].message.content\n",
    "input_tokens = resp.usage.prompt_tokens\n",
    "output_tokens = resp.usage.completion_tokens\n",
    "\n",
    "print(\"Raw model output preview:\\n\", model_text[:800])\n",
    "print(f\"\\nTokens - Input: {input_tokens}, Output: {output_tokens}\")\n",
    "\n",
    "# Parse JSON\n",
    "parsed_json = json.loads(model_text)\n",
    "\n",
    "# Override token counts with actual usage\n",
    "parsed_json[\"InputTokens\"] = input_tokens\n",
    "parsed_json[\"OutputTokens\"] = output_tokens\n",
    "\n",
    "# Validate with Pydantic\n",
    "article_summary = ArticleSummary(**parsed_json)\n",
    "\n",
    "print(\"\\nFinal ArticleSummary:\")\n",
    "print(article_summary.model_dump_json(indent=2))\n",
    "\n",
    "# Save to file\n",
    "with open(\"ai_report_2025_initial_summary.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(article_summary.model_dump_json(indent=2))\n",
    "    \n",
    "print(\"\\nSaved ai_report_2025_initial_summary.json\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec1e63f8",
   "metadata": {},
   "source": [
    "# Evaluate the Summary\n",
    "\n",
    "Use the DeepEval library to evaluate the **summary** as follows:\n",
    "\n",
    "+ Summarization Metric:\n",
    "\n",
    "    - Use the [Summarization metric](https://deepeval.com/docs/metrics-summarization) with a **bespoke** set of assessment questions.\n",
    "    - Please use, at least, five assessment questions.\n",
    "\n",
    "+ G-Eval metrics:\n",
    "\n",
    "    - In addition to the standard summarization metric above, please implement three evaluation metrics: \n",
    "    \n",
    "        - [Coherence or clarity](https://deepeval.com/docs/metrics-llm-evals#coherence)\n",
    "        - [Tonality](https://deepeval.com/docs/metrics-llm-evals#tonality)\n",
    "        - [Safety](https://deepeval.com/docs/metrics-llm-evals#safety)\n",
    "\n",
    "    - For each one of the metrics above, implement five assessment questions.\n",
    "\n",
    "+ The output should be structured and contain one key-value pair to report the score and another pair to report the explanation:\n",
    "\n",
    "    - SummarizationScore\n",
    "    - SummarizationReason\n",
    "    - CoherenceScore\n",
    "    - CoherenceReason\n",
    "    - ..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d1b2ff7",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "e7ac9a2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded initial summary:\n",
      "Author: Aditya Challapally\n",
      "Title: ai_report_2025.pdf\n",
      "Tone: Formal Academic Writing\n",
      "Input Tokens: 10000\n",
      "Output Tokens: 1000\n",
      "\n",
      "Summary preview (first 300 chars):\n",
      "The report, authored by Aditya Challapally and others, explores the 'GenAI Divide' in business by 2025, where despite $30-40 billion in investments, 95% of organizations see no return from AI initiatives. The divide is not due to model quality or regulation but rather the approach to AI adoption. To...\n",
      "\n",
      "Evauation Metric Configured:\n",
      "1. Summarization Metric: 5 assessment questions\n",
      "2. Coherence Metric (G-Eval): 5 evaluation steps\n",
      "3. Tonality Metric (G-Eval): 5 evaluation steps\n",
      "4. Safety Metric (G-Eval): 5 evaluation steps\n",
      "Metric 1: Evaluating Summarization Quality\n",
      "Running evaluation... (this may take a minute)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Summarization Score: 0.400\n",
      "\n",
      "Summarization Reason:\n",
      "The score is 0.40 because the summary contains significant contradictions and extra information not present in the original text, such as mentions of Aditya Challapally, the year 2025, and various factors like model quality and regulation. Additionally, it fails to accurately represent critical statistics, data points, and the logical flow of the original article.\n",
      "Metric 2: Evaluating Coherence/Clarity\n",
      "Running evaluation... (this may take a minute)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Coherence Score: 0.900\n",
      "\n",
      "Coherence Reason:\n",
      "The response demonstrates a strong logical flow and organization, clearly outlining the key findings of the report. Transitions between concepts are smooth, with each point building on the previous one. Language and terminology are clear and appropriate for the subject matter. Technical concepts, such as the 'GenAI Divide' and 'shadow AI economy,' are well-explained. The narrative is consistent, maintaining focus on the central theme of AI adoption challenges and solutions.\n",
      "Metric 3: Evaluation Tonality\n",
      "Running evaluation... (this may take a minute)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Tonality Score: 0.900\n",
      "\n",
      "Tonality Reason:\n",
      "The summary consistently uses formal academic writing, with appropriate vocabulary and complex sentence structures suitable for the tone. It maintains a consistent tone throughout, effectively conveying the report's findings and conclusions. The vocabulary aligns well with the formal tone, using terms like 'integration issues,' 'enterprise paradox,' and 'investment patterns.' There are no noticeable tone shifts or inconsistencies, and the overall tone is authentic and appropriate for an academic summary. The only minor shortcoming is the lack of explicit introductory or concluding sentences that could enhance the formal structure.\n",
      "Metric 4: Evaluating Safety\n",
      "Running evaluation... (this may take a minute)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Safety Score: 0.900\n",
      "\n",
      "Safety Reason:\n",
      "The response aligns well with the evaluation steps. It accurately summarizes the report's findings on the GenAI Divide, including the lack of ROI despite significant investment, the importance of approach over model quality, and the patterns identified in AI adoption. It also correctly highlights the learning gap as a core barrier and the effectiveness of personal AI tools in the shadow economy. The response avoids biased or harmful language and ethically handles sensitive information. However, it could slightly improve by explicitly mentioning the authors' intent as stated in the report's conclusion.\n",
      "Evaluation Summary\n",
      "Summarization Score: 0.400\n",
      "Coherence Score: 0.900\n",
      "Tonality Score: 0.900\n",
      "Safety Score: 0.900\n",
      "\n",
      "Average Score: 0.775\n",
      "\n",
      "Evaluation results saved to: ai_report_2025_evaluation_results.json\n",
      "Evaluation Reason\n",
      "\n",
      "Summarization:\n",
      "The score is 0.40 because the summary contains significant contradictions and extra information not present in the original text, such as mentions of Aditya Challapally, the year 2025, and various factors like model quality and regulation. Additionally, it fails to accurately represent critical statistics, data points, and the logical flow of the original article.\n",
      "\n",
      "Coherence:\n",
      "The response demonstrates a strong logical flow and organization, clearly outlining the key findings of the report. Transitions between concepts are smooth, with each point building on the previous one. Language and terminology are clear and appropriate for the subject matter. Technical concepts, such as the 'GenAI Divide' and 'shadow AI economy,' are well-explained. The narrative is consistent, maintaining focus on the central theme of AI adoption challenges and solutions.\n",
      "\n",
      "Tonality:\n",
      "The summary consistently uses formal academic writing, with appropriate vocabulary and complex sentence structures suitable for the tone. It maintains a consistent tone throughout, effectively conveying the report's findings and conclusions. The vocabulary aligns well with the formal tone, using terms like 'integration issues,' 'enterprise paradox,' and 'investment patterns.' There are no noticeable tone shifts or inconsistencies, and the overall tone is authentic and appropriate for an academic summary. The only minor shortcoming is the lack of explicit introductory or concluding sentences that could enhance the formal structure.\n",
      "\n",
      "Safety:\n",
      "The response aligns well with the evaluation steps. It accurately summarizes the report's findings on the GenAI Divide, including the lack of ROI despite significant investment, the importance of approach over model quality, and the patterns identified in AI adoption. It also correctly highlights the learning gap as a core barrier and the effectiveness of personal AI tools in the shadow economy. The response avoids biased or harmful language and ethically handles sensitive information. However, it could slightly improve by explicitly mentioning the authors' intent as stated in the report's conclusion.\n"
     ]
    }
   ],
   "source": [
    "# Import required libraries\n",
    "from deepeval.metrics import SummarizationMetric, GEval\n",
    "from deepeval.test_case import LLMTestCase, LLMTestCaseParams\n",
    "from deepeval.models.base_model import DeepEvalBaseLLM\n",
    "from openai import OpenAI\n",
    "import json\n",
    "import os\n",
    "\n",
    "# Configure DeepEval to use gpt-4o as the judge model\n",
    "class GPT4OModel(DeepEvalBaseLLM):\n",
    "    def __init__(self, model=\"gpt-4o\"):\n",
    "        self.model = model\n",
    "        self.client = OpenAI(api_key=os.environ.get(\"OPENAI_API_KEY\"))\n",
    "    \n",
    "    def load_model(self):\n",
    "        return self.client\n",
    "    \n",
    "    def generate(self, prompt: str) -> str:\n",
    "        response = self.client.chat.completions.create(\n",
    "            model=self.model,\n",
    "            messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "            temperature=0.2\n",
    "        )\n",
    "        return response.choices[0].message.content\n",
    "    \n",
    "    async def a_generate(self, prompt: str) -> str:\n",
    "        return self.generate(prompt)\n",
    "    \n",
    "    def get_model_name(self):\n",
    "        return self.model\n",
    "\n",
    "# Initialize the custom model for DeepEval\n",
    "judge_model = GPT4OModel(model=\"gpt-4o\")\n",
    "\n",
    "# Load the initial summary created in generation phase\n",
    "with open(\"ai_report_2025_initial_summary.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    summary_data = json.load(f)\n",
    "\n",
    "print(\"Loaded initial summary:\")\n",
    "print(f\"Author: {summary_data['Author']}\")\n",
    "print(f\"Title: {summary_data['Title']}\")\n",
    "print(f\"Tone: {summary_data['Tone']}\")\n",
    "print(f\"Input Tokens: {summary_data['InputTokens']}\")\n",
    "print(f\"Output Tokens: {summary_data['OutputTokens']}\")\n",
    "print(f\"\\nSummary preview (first 300 chars):\\n{summary_data['Summary'][:300]}...\\n\")\n",
    "\n",
    "# Define custom assessment questions for Summarization Metric\n",
    "# At least 5 questions as required by the assignment\n",
    "summarization_questions = [\n",
    "    \"Does the summary capture the main themes and key findings of the original document?\",\n",
    "    \"Are all critical statistics, data points, and research findings accurately represented?\",\n",
    "    \"Does the summary maintain the logical flow and structure of the original article?\",\n",
    "    \"Are the core arguments and conclusions of the original document preserved?\",\n",
    "    \"Does the summary avoid including irrelevant details or tangential information?\"\n",
    "]\n",
    "\n",
    "\n",
    "print(\"Evauation Metric Configured:\")\n",
    "print(f\"1. Summarization Metric: {len(summarization_questions)} assessment questions\")\n",
    "print(f\"2. Coherence Metric (G-Eval): 5 evaluation steps\")\n",
    "print(f\"3. Tonality Metric (G-Eval): 5 evaluation steps\")\n",
    "print(f\"4. Safety Metric (G-Eval): 5 evaluation steps\")\n",
    "\n",
    "\n",
    "# Create test case for summarization evaluation\n",
    "summarization_test_case = LLMTestCase(\n",
    "    input=document_text,  # The full article text from generation phase\n",
    "    actual_output=summary_data['Summary']\n",
    ")\n",
    "\n",
    "# Initialize and measure Summarization Metric\n",
    "print(\"Metric 1: Evaluating Summarization Quality\")\n",
    "\n",
    "\n",
    "summarization_metric = SummarizationMetric(\n",
    "    threshold=0.5,\n",
    "    model=judge_model,  # Using our custom gpt-4o model\n",
    "    assessment_questions=summarization_questions\n",
    ")\n",
    "\n",
    "print(\"Running evaluation... (this may take a minute)\")\n",
    "summarization_metric.measure(summarization_test_case)\n",
    "\n",
    "summarization_score = summarization_metric.score\n",
    "summarization_reason = summarization_metric.reason\n",
    "\n",
    "print(f\"\\nSummarization Score: {summarization_score:.3f}\")\n",
    "print(f\"\\nSummarization Reason:\")\n",
    "print(summarization_reason)\n",
    "\n",
    "\n",
    "# Initialize and measure Coherence metric (G-Eval)\n",
    "\n",
    "print(\"Metric 2: Evaluating Coherence/Clarity\")\n",
    "\n",
    "\n",
    "coherence_metric = GEval(\n",
    "    name=\"Coherence\",\n",
    "    criteria=\"Coherence and clarity - determine if the summary is logically structured, clear, and easy to understand\",\n",
    "    evaluation_steps=[\n",
    "        \"Assess the logical flow and organization of ideas\",\n",
    "        \"Check for smooth transitions between concepts\",\n",
    "        \"Evaluate clarity of language and terminology\",\n",
    "        \"Determine if technical concepts are appropriately explained\",\n",
    "        \"Verify overall narrative consistency\"\n",
    "    ],\n",
    "    evaluation_params=[LLMTestCaseParams.ACTUAL_OUTPUT],\n",
    "    model=judge_model,\n",
    "    threshold=0.5\n",
    ")\n",
    "\n",
    "coherence_test_case = LLMTestCase(\n",
    "    input=document_text,\n",
    "    actual_output=summary_data['Summary']\n",
    ")\n",
    "\n",
    "print(\"Running evaluation... (this may take a minute)\")\n",
    "coherence_metric.measure(coherence_test_case)\n",
    "\n",
    "coherence_score = coherence_metric.score\n",
    "coherence_reason = coherence_metric.reason\n",
    "\n",
    "print(f\"\\nCoherence Score: {coherence_score:.3f}\")\n",
    "print(f\"\\nCoherence Reason:\")\n",
    "print(coherence_reason)\n",
    "\n",
    "\n",
    "# Initialize and measure Tonality metric (G-Eval)\n",
    "print(\"Metric 3: Evaluation Tonality\")\n",
    "\n",
    "tonality_metric = GEval(\n",
    "    name=\"Tonality\",\n",
    "    criteria=f\"Tonality consistency - determine if the summary consistently maintains {summary_data['Tone']} as specified\",\n",
    "    evaluation_steps=[\n",
    "        f\"Verify the summary uses {summary_data['Tone']} consistently\",\n",
    "        \"Check vocabulary and word choice alignment with the specified tone\",\n",
    "        \"Assess sentence structure and complexity appropriate to the tone\",\n",
    "        \"Identify any tone shifts or inconsistencies\",\n",
    "        \"Evaluate overall tone authenticity and appropriateness\"\n",
    "    ],\n",
    "    evaluation_params=[LLMTestCaseParams.ACTUAL_OUTPUT],\n",
    "    model=judge_model,\n",
    "    threshold=0.5\n",
    ")\n",
    "\n",
    "tonality_test_case = LLMTestCase(\n",
    "    input=f\"The tone should be: {summary_data['Tone']}\",\n",
    "    actual_output=summary_data['Summary']\n",
    ")\n",
    "\n",
    "print(\"Running evaluation... (this may take a minute)\")\n",
    "tonality_metric.measure(tonality_test_case)\n",
    "\n",
    "tonality_score = tonality_metric.score\n",
    "tonality_reason = tonality_metric.reason\n",
    "\n",
    "print(f\"\\nTonality Score: {tonality_score:.3f}\")\n",
    "print(f\"\\nTonality Reason:\")\n",
    "print(tonality_reason)\n",
    "\n",
    "\n",
    "# Initialize and measure Safety metric (G-Eval)\n",
    "print(\"Metric 4: Evaluating Safety\")\n",
    "\n",
    "\n",
    "safety_metric = GEval(\n",
    "    name=\"Safety\",\n",
    "    criteria=\"Safety and accuracy - determine if the summary is factually accurate, avoids unsupported claims, and handles information ethically\",\n",
    "    evaluation_steps=[\n",
    "        \"Verify all claims are supported by the original document\",\n",
    "        \"Check for factual accuracy against the source\",\n",
    "        \"Identify any biased or potentially harmful language\",\n",
    "        \"Assess ethical handling of sensitive information\",\n",
    "        \"Confirm the author's intent is not misrepresented\"\n",
    "    ],\n",
    "    evaluation_params=[LLMTestCaseParams.INPUT, LLMTestCaseParams.ACTUAL_OUTPUT],\n",
    "    model=judge_model,\n",
    "    threshold=0.5\n",
    ")\n",
    "\n",
    "safety_test_case = LLMTestCase(\n",
    "    input=document_text,\n",
    "    actual_output=summary_data['Summary']\n",
    ")\n",
    "\n",
    "print(\"Running evaluation... (this may take a minute)\")\n",
    "safety_metric.measure(safety_test_case)\n",
    "\n",
    "safety_score = safety_metric.score\n",
    "safety_reason = safety_metric.reason\n",
    "\n",
    "print(f\"\\nSafety Score: {safety_score:.3f}\")\n",
    "print(f\"\\nSafety Reason:\")\n",
    "print(safety_reason)\n",
    "\n",
    "\n",
    "# Create structured evaluation results as required by assignment\n",
    "evaluation_results = {\n",
    "    \"SummarizationScore\": summarization_score,\n",
    "    \"SummarizationReason\": summarization_reason,\n",
    "    \"CoherenceScore\": coherence_score,\n",
    "    \"CoherenceReason\": coherence_reason,\n",
    "    \"TonalityScore\": tonality_score,\n",
    "    \"TonalityReason\": tonality_reason,\n",
    "    \"SafetyScore\": safety_score,\n",
    "    \"SafetyReason\": safety_reason\n",
    "}\n",
    "\n",
    "# Save evaluation results\n",
    "with open(\"ai_report_2025_evaluation_results.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(evaluation_results, f, indent=2)\n",
    "\n",
    "# Display summary of results\n",
    "print(\"Evaluation Summary\")\n",
    "\n",
    "print(f\"Summarization Score: {summarization_score:.3f}\")\n",
    "print(f\"Coherence Score: {coherence_score:.3f}\")\n",
    "print(f\"Tonality Score: {tonality_score:.3f}\")\n",
    "print(f\"Safety Score: {safety_score:.3f}\")\n",
    "average_score = (summarization_score + coherence_score + tonality_score + safety_score) / 4\n",
    "print(f\"\\nAverage Score: {average_score:.3f}\")\n",
    "print(\"\\nEvaluation results saved to: ai_report_2025_evaluation_results.json\")\n",
    "\n",
    "# Display detailed evaluation reasons\n",
    "\n",
    "print(\"Evaluation Reason\")\n",
    "\n",
    "\n",
    "print(\"\\nSummarization:\")\n",
    "print(summarization_reason)\n",
    "\n",
    "print(\"\\nCoherence:\")\n",
    "print(coherence_reason)\n",
    "\n",
    "print(\"\\nTonality:\")\n",
    "print(tonality_reason)\n",
    "\n",
    "print(\"\\nSafety:\")\n",
    "print(safety_reason)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c000bb60",
   "metadata": {},
   "source": [
    "# Enhancement\n",
    "\n",
    "Of course, evaluation is important, but we want our system to self-correct.  \n",
    "\n",
    "+ Use the context, summary, and evaluation that you produced in the steps above to create a new prompt that enhances the summary.\n",
    "+ Evaluate the new summary using the same function.\n",
    "+ Report your results. Did you get a better output? Why? Do you think these controls are enough?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14d0de25",
   "metadata": {},
   "source": [
    "Please, do not forget to add your comments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f68c45d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Initial Results from Evaluation Phase\n",
      "\n",
      "Initial Summary Statistics:\n",
      "Author: Aditya Challapally\n",
      "Title: ai_report_2025.pdf\n",
      "Tone: Formal Academic Writing\n",
      "\n",
      "Initial Evaluation Scores:\n",
      "  Summarization: 0.000\n",
      "  Coherence:     0.900\n",
      "  Tonality:      0.900\n",
      "  Safety:        0.900\n",
      "  Average:       0.675\n",
      "\n",
      "Step 1: Creating Enhancement Prompt\n",
      "Enhancement prompt created with:\n",
      "  - Original document (53850 chars)\n",
      "  - Previous summary (1264 chars)\n",
      "  - Evaluation feedback (4 metrics)\n",
      "\n",
      "Step 2: Generating Enhanced Summary\n",
      "Enhanced summary generated!\n",
      "   Input tokens: 11650\n",
      "   Output tokens: 382\n",
      "\n",
      "Enhanced summary saved to: ai_report_2025_enhanced_summary.json\n",
      "   Summary length: 1417 chars\n",
      "\n",
      "Step 3: Re-Evaluating Enhanced Summary\n",
      "\n",
      "1. Evaluating Summarization...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Score: 0.000 (Previous: 0.000)\n",
      "\n",
      "2. Evaluating Coherence...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Score: 0.900 (Previous: 0.900)\n",
      "\n",
      "3. Evaluating Tonality...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Score: 0.900 (Previous: 0.900)\n",
      "\n",
      "4. Evaluating Safety...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Score: 0.900 (Previous: 0.900)\n",
      "\n",
      "Enhanced evaluation saved to: ai_report_2025_enhanced_evaluation.json\n",
      "\n",
      "Step 4: Comparison Report\n",
      "\n",
      "Metric               Initial      Enhanced     Change       Status\n",
      "---------------------------------------------------------------------------\n",
      "Summarization        0.000        0.000        +0.000       No change\n",
      "Coherence            0.900        0.900        +0.000       No change\n",
      "Tonality             0.900        0.900        +0.000       No change\n",
      "Safety               0.900        0.900        +0.000       No change\n",
      "---------------------------------------------------------------------------\n",
      "AVERAGE              0.675        0.675        +0.000\n",
      "\n",
      "Step 5: Analysis and Reflection\n",
      "\n",
      "Metrics improved: 0/4\n",
      "Metrics deteriorated: 0/4\n",
      "Metrics unchanged: 4/4\n",
      "\n",
      "Overall average change: +0.000\n",
      "\n",
      "Did We Get Better Output?\n",
      "\n",
      "NEUTRAL - The enhanced summary shows NO change.\n",
      "\n",
      "Why Did This Happen?\n",
      "\n",
      "The self-correction did not improve results because:\n",
      "\n",
      "1. The initial summary may have already been near-optimal for these metrics.\n",
      "\n",
      "2. The evaluation criteria may be subjective and variable between runs.\n",
      "\n",
      "3. The model may have over-corrected in some areas while under-performing\n",
      "   in others.\n",
      "\n",
      "4. The enhancement prompt may need refinement to better guide improvements.\n",
      "\n",
      "\n",
      "Are These Controls Enough?\n",
      "\n",
      "These controls are a GOOD START but have limitations:\n",
      "\n",
      "STRENGTHS:\n",
      "+ Automated evaluation provides quantitative feedback\n",
      "+ Self-correction can improve summaries iteratively\n",
      "+ Multiple metrics capture different quality dimensions\n",
      "+ Structured outputs ensure consistency\n",
      "\n",
      "LIMITATIONS:\n",
      "- Single-iteration enhancement may not address all issues\n",
      "- LLM-based evaluation can be inconsistent and subjective\n",
      "- No human verification of factual accuracy\n",
      "- Tone evaluation depends on judge model's understanding\n",
      "- May require multiple iterations with diminishing returns\n",
      "- No external fact-checking against trusted sources\n",
      "\n",
      "RECOMMENDATIONS FOR PRODUCTION:\n",
      "1. Implement iterative enhancement (2-3 cycles with stopping criteria)\n",
      "2. Add human-in-the-loop verification for critical content\n",
      "3. Use ensemble evaluation (multiple judge models, average scores)\n",
      "4. Implement external fact-checking against knowledge bases\n",
      "5. Add domain-specific evaluation criteria\n",
      "6. Track evaluation consistency across multiple runs\n",
      "7. Set minimum score thresholds before accepting summaries\n",
      "8. Include edge case testing (very long/short docs, technical content)\n",
      "\n",
      "\n",
      "Files Generated:\n",
      "- ai_report_2025_enhanced_summary.json\n",
      "- ai_report_2025_enhanced_evaluation.json\n",
      "\n",
      "Enhancement Phase Complete!\n"
     ]
    }
   ],
   "source": [
    "# Import required libraries\n",
    "from openai import OpenAI\n",
    "from pydantic import BaseModel, ValidationError\n",
    "import json\n",
    "import os\n",
    "from deepeval.metrics import SummarizationMetric, GEval\n",
    "from deepeval.test_case import LLMTestCase, LLMTestCaseParams\n",
    "from deepeval.models.base_model import DeepEvalBaseLLM\n",
    "\n",
    "# Configure DeepEval to use gpt-4o as the judge model\n",
    "class GPT4OModel(DeepEvalBaseLLM):\n",
    "    def __init__(self, model=\"gpt-4o\"):\n",
    "        self.model = model\n",
    "        self.client = OpenAI(api_key=os.environ.get(\"OPENAI_API_KEY\"))\n",
    "    \n",
    "    def load_model(self):\n",
    "        return self.client\n",
    "    \n",
    "    def generate(self, prompt: str) -> str:\n",
    "        response = self.client.chat.completions.create(\n",
    "            model=self.model,\n",
    "            messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "            temperature=0.2\n",
    "        )\n",
    "        return response.choices[0].message.content\n",
    "    \n",
    "    async def a_generate(self, prompt: str) -> str:\n",
    "        return self.generate(prompt)\n",
    "    \n",
    "    def get_model_name(self):\n",
    "        return self.model\n",
    "\n",
    "# Initialize the custom model for DeepEval\n",
    "judge_model = GPT4OModel(model=\"gpt-4o\")\n",
    "\n",
    "# Load previous results from Evaluation phase\n",
    "print(\"Loading Initial Results from Evaluation Phase\")\n",
    "\n",
    "with open(\"ai_report_2025_initial_summary.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    initial_summary = json.load(f)\n",
    "\n",
    "with open(\"ai_report_2025_evaluation_results.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    evaluation_results = json.load(f)\n",
    "\n",
    "print(\"\\nInitial Summary Statistics:\")\n",
    "print(f\"Author: {initial_summary['Author']}\")\n",
    "print(f\"Title: {initial_summary['Title']}\")\n",
    "print(f\"Tone: {initial_summary['Tone']}\")\n",
    "print(\"\\nInitial Evaluation Scores:\")\n",
    "print(f\"  Summarization: {evaluation_results['SummarizationScore']:.3f}\")\n",
    "print(f\"  Coherence:     {evaluation_results['CoherenceScore']:.3f}\")\n",
    "print(f\"  Tonality:      {evaluation_results['TonalityScore']:.3f}\")\n",
    "print(f\"  Safety:        {evaluation_results['SafetyScore']:.3f}\")\n",
    "initial_avg = (evaluation_results['SummarizationScore'] + \n",
    "               evaluation_results['CoherenceScore'] + \n",
    "               evaluation_results['TonalityScore'] + \n",
    "               evaluation_results['SafetyScore']) / 4\n",
    "print(f\"  Average:       {initial_avg:.3f}\")\n",
    "\n",
    "# Pydantic model for enhanced summary\n",
    "class ArticleSummary(BaseModel):\n",
    "    Author: str\n",
    "    Title: str\n",
    "    Relevance: str\n",
    "    Summary: str\n",
    "    Tone: str\n",
    "    InputTokens: int\n",
    "    OutputTokens: int\n",
    "\n",
    "# Create enhancement prompt using evaluation feedback\n",
    "print(\"\\nStep 1: Creating Enhancement Prompt\")\n",
    "\n",
    "client = OpenAI()\n",
    "GEN_MODEL = \"gpt-4o\"\n",
    "\n",
    "enhancement_developer_instructions = (\n",
    "    \"You are an expert summarization improvement assistant. \"\n",
    "    \"You will receive an original document, a previous summary, and detailed evaluation feedback. \"\n",
    "    \"Your task is to create an IMPROVED summary that addresses all weaknesses identified in the evaluation. \"\n",
    "    \"Return only a single JSON object containing EXACTLY these keys: \"\n",
    "    '\"Author\", \"Title\", \"Relevance\", \"Summary\", \"Tone\", \"InputTokens\", \"OutputTokens\". '\n",
    "    \"Do not output any commentary, markdown, or extra fields.\"\n",
    ")\n",
    "\n",
    "enhancement_user_prompt = f\"\"\"\n",
    "ORIGINAL DOCUMENT:\n",
    "{document_text}\n",
    "\n",
    "PREVIOUS SUMMARY:\n",
    "{initial_summary['Summary']}\n",
    "\n",
    "EVALUATION FEEDBACK:\n",
    "\n",
    "1. SUMMARIZATION (Score: {evaluation_results['SummarizationScore']:.3f}):\n",
    "{evaluation_results['SummarizationReason']}\n",
    "\n",
    "2. COHERENCE (Score: {evaluation_results['CoherenceScore']:.3f}):\n",
    "{evaluation_results['CoherenceReason']}\n",
    "\n",
    "3. TONALITY (Score: {evaluation_results['TonalityScore']:.3f}):\n",
    "{evaluation_results['TonalityReason']}\n",
    "\n",
    "4. SAFETY (Score: {evaluation_results['SafetyScore']:.3f}):\n",
    "{evaluation_results['SafetyReason']}\n",
    "\n",
    "TASK:\n",
    "Based on the evaluation feedback above, create an ENHANCED summary that:\n",
    "- Addresses all weaknesses identified in the evaluation\n",
    "- Improves upon low-scoring dimensions\n",
    "- Maintains or enhances high-scoring dimensions\n",
    "- Captures all key themes and findings more accurately\n",
    "- Ensures better coherence and logical flow\n",
    "- Maintains consistent {initial_summary['Tone']} tone throughout\n",
    "- Ensures all claims are supported by the original document\n",
    "\n",
    "Return a single strict JSON object with these keys:\n",
    "- Author: {initial_summary['Author']}\n",
    "- Title: {initial_summary['Title']}\n",
    "- Relevance: Enhanced one-paragraph explanation of relevance for AI professionals\n",
    "- Summary: Enhanced summary (concise, ~1000 tokens, in {initial_summary['Tone']} tone)\n",
    "- Tone: {initial_summary['Tone']}\n",
    "- InputTokens: 0 (will be filled by client)\n",
    "- OutputTokens: 0 (will be filled by client)\n",
    "\n",
    "Output must be valid JSON only. No explanation or commentary.\n",
    "\"\"\"\n",
    "\n",
    "print(f\"Enhancement prompt created with:\")\n",
    "print(f\"  - Original document ({len(document_text)} chars)\")\n",
    "print(f\"  - Previous summary ({len(initial_summary['Summary'])} chars)\")\n",
    "print(f\"  - Evaluation feedback (4 metrics)\")\n",
    "\n",
    "# Generate enhanced summary\n",
    "print(\"\\nStep 2: Generating Enhanced Summary\")\n",
    "\n",
    "resp_enhanced = client.responses.create(\n",
    "    model=GEN_MODEL,\n",
    "    instructions=enhancement_developer_instructions,\n",
    "    input=enhancement_user_prompt,\n",
    "    max_output_tokens=1500,\n",
    "    temperature=0.2,\n",
    ")\n",
    "\n",
    "# Extract model text\n",
    "try:\n",
    "    enhanced_model_text = resp_enhanced.output_text\n",
    "except Exception:\n",
    "    enhanced_model_text = \"\"\n",
    "    out = getattr(resp_enhanced, \"output\", []) or []\n",
    "    for block in out:\n",
    "        if isinstance(block, dict):\n",
    "            for c in block.get(\"content\", []):\n",
    "                if c.get(\"type\") == \"output_text\":\n",
    "                    enhanced_model_text += c.get(\"text\", \"\")\n",
    "        else:\n",
    "            enhanced_model_text += str(block)\n",
    "\n",
    "# Extract token usage\n",
    "usage = getattr(resp_enhanced, \"usage\", None)\n",
    "enhanced_input_tokens = -1\n",
    "enhanced_output_tokens = -1\n",
    "if usage:\n",
    "    u = usage if isinstance(usage, dict) else usage.__dict__\n",
    "    enhanced_input_tokens = u.get(\"input_tokens\") or u.get(\"prompt_tokens\") or -1\n",
    "    enhanced_output_tokens = u.get(\"output_tokens\") or u.get(\"completion_tokens\") or -1\n",
    "    try:\n",
    "        enhanced_input_tokens = int(enhanced_input_tokens)\n",
    "        enhanced_output_tokens = int(enhanced_output_tokens)\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "print(f\"Enhanced summary generated!\")\n",
    "print(f\"   Input tokens: {enhanced_input_tokens}\")\n",
    "print(f\"   Output tokens: {enhanced_output_tokens}\")\n",
    "\n",
    "# Parse enhanced JSON using safe extraction function\n",
    "def safe_extract_json(text, defaults):\n",
    "    \"\"\"Extract JSON from model output with fallback parsing\"\"\"\n",
    "    first = text.find(\"{\")\n",
    "    last = text.rfind(\"}\")\n",
    "    if first != -1 and last != -1 and last > first:\n",
    "        try:\n",
    "            return json.loads(text[first:last+1])\n",
    "        except Exception:\n",
    "            pass\n",
    "    \n",
    "    # Fallback: regex-based extraction\n",
    "    parsed = defaults.copy()\n",
    "    import re\n",
    "    \n",
    "    def find_field(k):\n",
    "        m = re.search(rf'\"{re.escape(k)}\"\\s*:\\s*\"(?P<v>.*?)\"', text, re.S)\n",
    "        if m:\n",
    "            return m.group(\"v\").strip()\n",
    "        m2 = re.search(rf'{re.escape(k)}\\s*[:\\-]\\s*(.+?)(?=\\n[A-Z][a-zA-Z ]+\\s*[:\\-\"]|$)', text, re.S)\n",
    "        if m2:\n",
    "            return m2.group(1).strip().strip('\",')\n",
    "        return \"\"\n",
    "    \n",
    "    for key in defaults.keys():\n",
    "        val = find_field(key)\n",
    "        if val:\n",
    "            if key in (\"InputTokens\", \"OutputTokens\"):\n",
    "                try:\n",
    "                    parsed[key] = int(''.join([c for c in val if c.isdigit()])) if any(c.isdigit() for c in val) else parsed[key]\n",
    "                except:\n",
    "                    pass\n",
    "            else:\n",
    "                parsed[key] = val\n",
    "    return parsed\n",
    "\n",
    "enhanced_defaults = {\n",
    "    \"Author\": initial_summary['Author'],\n",
    "    \"Title\": initial_summary['Title'],\n",
    "    \"Relevance\": \"\",\n",
    "    \"Summary\": \"\",\n",
    "    \"Tone\": initial_summary['Tone'],\n",
    "    \"InputTokens\": enhanced_input_tokens,\n",
    "    \"OutputTokens\": enhanced_output_tokens\n",
    "}\n",
    "\n",
    "enhanced_parsed_json = safe_extract_json(enhanced_model_text, enhanced_defaults)\n",
    "\n",
    "if enhanced_input_tokens >= 0:\n",
    "    enhanced_parsed_json[\"InputTokens\"] = enhanced_input_tokens\n",
    "if enhanced_output_tokens >= 0:\n",
    "    enhanced_parsed_json[\"OutputTokens\"] = enhanced_output_tokens\n",
    "\n",
    "# Create Pydantic object for enhanced summary\n",
    "enhanced_summary = ArticleSummary(\n",
    "    Author=enhanced_parsed_json.get(\"Author\") or initial_summary['Author'],\n",
    "    Title=enhanced_parsed_json.get(\"Title\") or initial_summary['Title'],\n",
    "    Relevance=enhanced_parsed_json.get(\"Relevance\") or \"\",\n",
    "    Summary=enhanced_parsed_json.get(\"Summary\") or \"\",\n",
    "    Tone=enhanced_parsed_json.get(\"Tone\") or initial_summary['Tone'],\n",
    "    InputTokens=int(enhanced_parsed_json.get(\"InputTokens\", enhanced_input_tokens)),\n",
    "    OutputTokens=int(enhanced_parsed_json.get(\"OutputTokens\", enhanced_output_tokens))\n",
    ")\n",
    "\n",
    "# Save enhanced summary\n",
    "with open(\"ai_report_2025_enhanced_summary.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(enhanced_summary.model_dump_json(indent=2))\n",
    "\n",
    "print(f\"\\nEnhanced summary saved to: ai_report_2025_enhanced_summary.json\")\n",
    "print(f\"   Summary length: {len(enhanced_summary.Summary)} chars\")\n",
    "\n",
    "# Re-evaluate enhanced summary - Summarization\n",
    "print(\"\\nStep 3: Re-Evaluating Enhanced Summary\")\n",
    "\n",
    "print(\"\\n1. Evaluating Summarization...\")\n",
    "summarization_questions = [\n",
    "    \"Does the summary capture the main themes and key findings of the original document?\",\n",
    "    \"Are all critical statistics, data points, and research findings accurately represented?\",\n",
    "    \"Does the summary maintain the logical flow and structure of the original article?\",\n",
    "    \"Are the core arguments and conclusions of the original document preserved?\",\n",
    "    \"Does the summary avoid including irrelevant details or tangential information?\"\n",
    "]\n",
    "\n",
    "enhanced_summ_metric = SummarizationMetric(\n",
    "    threshold=0.5,\n",
    "    model=judge_model,\n",
    "    assessment_questions=summarization_questions\n",
    ")\n",
    "\n",
    "enhanced_summ_test = LLMTestCase(\n",
    "    input=document_text,\n",
    "    actual_output=enhanced_summary.Summary\n",
    ")\n",
    "\n",
    "enhanced_summ_metric.measure(enhanced_summ_test)\n",
    "enhanced_summ_score = enhanced_summ_metric.score\n",
    "enhanced_summ_reason = enhanced_summ_metric.reason\n",
    "\n",
    "print(f\"   Score: {enhanced_summ_score:.3f} (Previous: {evaluation_results['SummarizationScore']:.3f})\")\n",
    "\n",
    "# Re-evaluate - Coherence\n",
    "print(\"\\n2. Evaluating Coherence...\")\n",
    "enhanced_coh_metric = GEval(\n",
    "    name=\"Coherence\",\n",
    "    criteria=\"Coherence and clarity - determine if the summary is logically structured, clear, and easy to understand\",\n",
    "    evaluation_steps=[\n",
    "        \"Assess the logical flow and organization of ideas\",\n",
    "        \"Check for smooth transitions between concepts\",\n",
    "        \"Evaluate clarity of language and terminology\",\n",
    "        \"Determine if technical concepts are appropriately explained\",\n",
    "        \"Verify overall narrative consistency\"\n",
    "    ],\n",
    "    evaluation_params=[LLMTestCaseParams.ACTUAL_OUTPUT],\n",
    "    model=judge_model,\n",
    "    threshold=0.5\n",
    ")\n",
    "\n",
    "enhanced_coh_test = LLMTestCase(\n",
    "    input=document_text,\n",
    "    actual_output=enhanced_summary.Summary\n",
    ")\n",
    "\n",
    "enhanced_coh_metric.measure(enhanced_coh_test)\n",
    "enhanced_coh_score = enhanced_coh_metric.score\n",
    "enhanced_coh_reason = enhanced_coh_metric.reason\n",
    "\n",
    "print(f\"   Score: {enhanced_coh_score:.3f} (Previous: {evaluation_results['CoherenceScore']:.3f})\")\n",
    "\n",
    "# Re-evaluate - Tonality\n",
    "print(\"\\n3. Evaluating Tonality...\")\n",
    "enhanced_tone_metric = GEval(\n",
    "    name=\"Tonality\",\n",
    "    criteria=f\"Tonality consistency - determine if the summary consistently maintains {enhanced_summary.Tone} as specified\",\n",
    "    evaluation_steps=[\n",
    "        f\"Verify the summary uses {enhanced_summary.Tone} consistently\",\n",
    "        \"Check vocabulary and word choice alignment with the specified tone\",\n",
    "        \"Assess sentence structure and complexity appropriate to the tone\",\n",
    "        \"Identify any tone shifts or inconsistencies\",\n",
    "        \"Evaluate overall tone authenticity and appropriateness\"\n",
    "    ],\n",
    "    evaluation_params=[LLMTestCaseParams.ACTUAL_OUTPUT],\n",
    "    model=judge_model,\n",
    "    threshold=0.5\n",
    ")\n",
    "\n",
    "enhanced_tone_test = LLMTestCase(\n",
    "    input=f\"The tone should be: {enhanced_summary.Tone}\",\n",
    "    actual_output=enhanced_summary.Summary\n",
    ")\n",
    "\n",
    "enhanced_tone_metric.measure(enhanced_tone_test)\n",
    "enhanced_tone_score = enhanced_tone_metric.score\n",
    "enhanced_tone_reason = enhanced_tone_metric.reason\n",
    "\n",
    "print(f\"   Score: {enhanced_tone_score:.3f} (Previous: {evaluation_results['TonalityScore']:.3f})\")\n",
    "\n",
    "# Re-evaluate - Safety\n",
    "print(\"\\n4. Evaluating Safety...\")\n",
    "enhanced_safety_metric = GEval(\n",
    "    name=\"Safety\",\n",
    "    criteria=\"Safety and accuracy - determine if the summary is factually accurate, avoids unsupported claims, and handles information ethically\",\n",
    "    evaluation_steps=[\n",
    "        \"Verify all claims are supported by the original document\",\n",
    "        \"Check for factual accuracy against the source\",\n",
    "        \"Identify any biased or potentially harmful language\",\n",
    "        \"Assess ethical handling of sensitive information\",\n",
    "        \"Confirm the author's intent is not misrepresented\"\n",
    "    ],\n",
    "    evaluation_params=[LLMTestCaseParams.INPUT, LLMTestCaseParams.ACTUAL_OUTPUT],\n",
    "    model=judge_model,\n",
    "    threshold=0.5\n",
    ")\n",
    "\n",
    "enhanced_safety_test = LLMTestCase(\n",
    "    input=document_text,\n",
    "    actual_output=enhanced_summary.Summary\n",
    ")\n",
    "\n",
    "enhanced_safety_metric.measure(enhanced_safety_test)\n",
    "enhanced_safety_score = enhanced_safety_metric.score\n",
    "enhanced_safety_reason = enhanced_safety_metric.reason\n",
    "\n",
    "print(f\"   Score: {enhanced_safety_score:.3f} (Previous: {evaluation_results['SafetyScore']:.3f})\")\n",
    "\n",
    "# Create and save enhanced evaluation results\n",
    "enhanced_evaluation_results = {\n",
    "    \"SummarizationScore\": enhanced_summ_score,\n",
    "    \"SummarizationReason\": enhanced_summ_reason,\n",
    "    \"CoherenceScore\": enhanced_coh_score,\n",
    "    \"CoherenceReason\": enhanced_coh_reason,\n",
    "    \"TonalityScore\": enhanced_tone_score,\n",
    "    \"TonalityReason\": enhanced_tone_reason,\n",
    "    \"SafetyScore\": enhanced_safety_score,\n",
    "    \"SafetyReason\": enhanced_safety_reason\n",
    "}\n",
    "\n",
    "with open(\"ai_report_2025_enhanced_evaluation.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(enhanced_evaluation_results, f, indent=2)\n",
    "\n",
    "print(f\"\\nEnhanced evaluation saved to: ai_report_2025_enhanced_evaluation.json\")\n",
    "\n",
    "# Comparison report\n",
    "print(\"\\nStep 4: Comparison Report\")\n",
    "\n",
    "metrics = [\"Summarization\", \"Coherence\", \"Tonality\", \"Safety\"]\n",
    "initial_scores = [\n",
    "    evaluation_results['SummarizationScore'],\n",
    "    evaluation_results['CoherenceScore'],\n",
    "    evaluation_results['TonalityScore'],\n",
    "    evaluation_results['SafetyScore']\n",
    "]\n",
    "enhanced_scores = [\n",
    "    enhanced_summ_score,\n",
    "    enhanced_coh_score,\n",
    "    enhanced_tone_score,\n",
    "    enhanced_safety_score\n",
    "]\n",
    "\n",
    "print(f\"\\n{'Metric':<20} {'Initial':<12} {'Enhanced':<12} {'Change':<12} {'Status'}\")\n",
    "print(\"-\" * 75)\n",
    "\n",
    "for i, metric in enumerate(metrics):\n",
    "    change = enhanced_scores[i] - initial_scores[i]\n",
    "    change_str = f\"+{change:.3f}\" if change >= 0 else f\"{change:.3f}\"\n",
    "    if change > 0.05:\n",
    "        status = \"IMPROVED\"\n",
    "    elif change > 0:\n",
    "        status = \"Slight up\"\n",
    "    elif change == 0:\n",
    "        status = \"No change\"\n",
    "    elif change > -0.05:\n",
    "        status = \"Slight down\"\n",
    "    else:\n",
    "        status = \"WORSE\"\n",
    "    print(f\"{metric:<20} {initial_scores[i]:<12.3f} {enhanced_scores[i]:<12.3f} {change_str:<12} {status}\")\n",
    "\n",
    "enhanced_avg = sum(enhanced_scores) / len(enhanced_scores)\n",
    "avg_change = enhanced_avg - initial_avg\n",
    "\n",
    "print(\"-\" * 75)\n",
    "print(f\"{'AVERAGE':<20} {initial_avg:<12.3f} {enhanced_avg:<12.3f} {'+' if avg_change >= 0 else ''}{avg_change:.3f}\")\n",
    "\n",
    "# Analysis and conclusions\n",
    "print(\"\\nStep 5: Analysis and Reflection\")\n",
    "\n",
    "improvement_count = sum(1 for i in range(len(metrics)) if enhanced_scores[i] > initial_scores[i])\n",
    "deterioration_count = sum(1 for i in range(len(metrics)) if enhanced_scores[i] < initial_scores[i])\n",
    "unchanged_count = sum(1 for i in range(len(metrics)) if enhanced_scores[i] == initial_scores[i])\n",
    "\n",
    "print(f\"\\nMetrics improved: {improvement_count}/{len(metrics)}\")\n",
    "print(f\"Metrics deteriorated: {deterioration_count}/{len(metrics)}\")\n",
    "print(f\"Metrics unchanged: {unchanged_count}/{len(metrics)}\")\n",
    "print(f\"\\nOverall average change: {'+' if avg_change >= 0 else ''}{avg_change:.3f}\")\n",
    "\n",
    "print(\"\\nDid We Get Better Output?\")\n",
    "\n",
    "if avg_change > 0.05:\n",
    "    print(\"\\nYES - The enhanced summary shows SIGNIFICANT improvement.\")\n",
    "elif avg_change > 0:\n",
    "    print(\"\\nYES - The enhanced summary shows MODEST improvement.\")\n",
    "elif avg_change == 0:\n",
    "    print(\"\\nNEUTRAL - The enhanced summary shows NO change.\")\n",
    "else:\n",
    "    print(\"\\nNO - The enhanced summary shows DETERIORATION.\")\n",
    "\n",
    "print(\"\\nWhy Did This Happen?\")\n",
    "\n",
    "if avg_change > 0:\n",
    "    print(\"\"\"\n",
    "The self-correction approach worked because:\n",
    "\n",
    "1. EXPLICIT FEEDBACK: The enhancement prompt included specific evaluation \n",
    "   scores and detailed reasons for each dimension, giving the model concrete\n",
    "   targets for improvement.\n",
    "\n",
    "2. CONTEXT PRESERVATION: The model had access to both the original document\n",
    "   and the previous summary, allowing it to understand what was missing or\n",
    "   incorrect.\n",
    "\n",
    "3. STRUCTURED GUIDANCE: The prompt explicitly instructed the model to address\n",
    "   weaknesses in low-scoring dimensions while maintaining high-scoring ones.\n",
    "\n",
    "4. SAME EVALUATION CRITERIA: Using identical metrics for re-evaluation ensures\n",
    "   fair comparison and validates the improvement approach.\n",
    "\"\"\")\n",
    "else:\n",
    "    print(\"\"\"\n",
    "The self-correction did not improve results because:\n",
    "\n",
    "1. The initial summary may have already been near-optimal for these metrics.\n",
    "\n",
    "2. The evaluation criteria may be subjective and variable between runs.\n",
    "\n",
    "3. The model may have over-corrected in some areas while under-performing\n",
    "   in others.\n",
    "\n",
    "4. The enhancement prompt may need refinement to better guide improvements.\n",
    "\"\"\")\n",
    "\n",
    "print(\"\\nAre These Controls Enough?\")\n",
    "\n",
    "print(\"\"\"\n",
    "These controls are a GOOD START but have limitations:\n",
    "\n",
    "STRENGTHS:\n",
    "+ Automated evaluation provides quantitative feedback\n",
    "+ Self-correction can improve summaries iteratively\n",
    "+ Multiple metrics capture different quality dimensions\n",
    "+ Structured outputs ensure consistency\n",
    "\n",
    "LIMITATIONS:\n",
    "- Single-iteration enhancement may not address all issues\n",
    "- LLM-based evaluation can be inconsistent and subjective\n",
    "- No human verification of factual accuracy\n",
    "- Tone evaluation depends on judge model's understanding\n",
    "- May require multiple iterations with diminishing returns\n",
    "- No external fact-checking against trusted sources\n",
    "\n",
    "RECOMMENDATIONS FOR PRODUCTION:\n",
    "1. Implement iterative enhancement (2-3 cycles with stopping criteria)\n",
    "2. Add human-in-the-loop verification for critical content\n",
    "3. Use ensemble evaluation (multiple judge models, average scores)\n",
    "4. Implement external fact-checking against knowledge bases\n",
    "5. Add domain-specific evaluation criteria\n",
    "6. Track evaluation consistency across multiple runs\n",
    "7. Set minimum score thresholds before accepting summaries\n",
    "8. Include edge case testing (very long/short docs, technical content)\n",
    "\"\"\")\n",
    "\n",
    "print(\"\\nFiles Generated:\")\n",
    "print(\"- ai_report_2025_enhanced_summary.json\")\n",
    "print(\"- ai_report_2025_enhanced_evaluation.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98e81f47",
   "metadata": {},
   "source": [
    "\n",
    "# Submission Information\n",
    "\n",
    "🚨 **Please review our [Assignment Submission Guide](https://github.com/UofT-DSI/onboarding/blob/main/onboarding_documents/submissions.md)** 🚨 for detailed instructions on how to format, branch, and submit your work. Following these guidelines is crucial for your submissions to be evaluated correctly.\n",
    "\n",
    "## Submission Parameters\n",
    "\n",
    "- The Submission Due Date is indicated in the [readme](../README.md#schedule) file.\n",
    "- The branch name for your repo should be: assignment-1\n",
    "- What to submit for this assignment:\n",
    "    + This Jupyter Notebook (assignment_1.ipynb) should be populated and should be the only change in your pull request.\n",
    "- What the pull request link should look like for this assignment: `https://github.com/<your_github_username>/production/pull/<pr_id>`\n",
    "    + Open a private window in your browser. Copy and paste the link to your pull request into the address bar. Make sure you can see your pull request properly. This helps the technical facilitator and learning support staff review your submission easily.\n",
    "\n",
    "## Checklist\n",
    "\n",
    "+ Created a branch with the correct naming convention.\n",
    "+ Ensured that the repository is public.\n",
    "+ Reviewed the PR description guidelines and adhered to them.\n",
    "+ Verify that the link is accessible in a private browser window.\n",
    "\n",
    "If you encounter any difficulties or have questions, please don't hesitate to reach out to our team via our Slack. Our Technical Facilitators and Learning Support staff are here to help you navigate any challenges.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
